---
publishDate: 2025-11-19 14:59:14
title: 高统期末复习
description: 高统期末复习
tags:
  - 笔记
comment: true
---

## 一些常见术语的解释

### 方差
$$
E(\sum_{i=1}^n(X-\mu)^2)
$$
- 对于根据训练数据集训练出的模型，其方差越大，对于训练集的微小变化就越敏感，泛化能力越弱
- 方差体现的是所设定模型自身的性质（对训练集数据的敏感性）
- 自由度越高的模型其方差越高，偏差越小
### 残差平方和 RSS (Residual Sum of Squares) 
$$
\sum_{i=1}^n (y_i - \hat{y}_i)^2
$$
用来衡量误差的指标
### 最小二乘法
通过**寻找最小的 RSS**，寻找拟合度最高的线
### 均方误差 MSE (mean squared error) 
对于数据集 Tr：
$$
MSE_{T_r}=Ave_{i∈{T_r}}[y_i-\hat{f}(x_i)]=\frac{RSS}{n}
$$
是回归中最常用的评价准则
> [!note]
> 为什么不用 RSS？因为数据集越大，RSS 就会越大，不能跨数据集，相比之下 MSE 更直观、不用考虑数据集大小
### 方差 vs 偏差
- 偏差是不可消除的，体现的是“假设”的模型与“真实”模型之间的本质差异
- 方差越大，说明我们拟合的模型对输入数据越敏感，细微的变化就会得到不同的拟合输出结果

- 模型太简单 → **高偏差、低方差** → 拟合不够    
- 模型太复杂 → **低偏差、高方差** → 过拟合
所以**平衡偏差/误差**就是一个重要问题
### 标准误差 SE (Standard Error)
- 估计量的标准差，表示这个估计在**重复抽样时会晃动的多厉害**
- 实际上也就是重复抽样得到的每次结果的标准差
- 标准误差越小，说明估计的“抖动”越小，越精准
$$
SE=\frac{s}{\sqrt{n}}
$$
### 残差标准误差 RSE (Residual Standard Error)
在线性回归中：
$$
y=X\beta+\epsilon,\epsilon\sim
N(0,\sigma^2)
$$
RSE 是对误差项标准差 $\sigma$ 的估计：
$$
RSE=\hat{\sigma}=\sqrt{\frac{RSS}{df}}=\sqrt{\frac{RSS}{n-p-1}}
$$
其中 p+1 是参数个数（含截距）
RSE 统计的是误差项的标准差 $\sigma$，体现的是**模型的整体噪声大小**
> [!note]
> - 标准差（SD）反应的是**数据的波动程度**，是方差开根号的结果
> - 标准误差（SE）是在数据中随机抽 n 个样本，n 个样本的估计量的标准差，反应的是**估计的稳定程度**
> - 残差是估计值与实际值的差值
> - 残差标准误差（RSE）是对误差项 $\epsilon$ 标准差（SD）的估计
> 总结：
> 	- 标准差(SD)：数据的乱度
> 	- 标准误差(SE)：估计的晃动程度
> 	- 残差标准误差(RSE)：回归模型噪声的典型大小

### t 统计量
$$
t=\frac{\bar{X}-\mu_0}{SE}
$$
比较样本均值与总体均值的差异
### t 值（判断H0 假设，绝对值越大，假设越不成立）
线性回归中，我们要判断某一个系数究竟是不是 0（有没有关系），常采用零假设 $H_0$，而 t 值就是用来判断这个假设检验的。
公式：
$$
t_j=\frac{\hat{\beta}_j-\beta_{j,0}}{SE(\hat{\beta}_j)}
$$
其中：
$\hat{\beta}_j$ 表示估计值
$\beta_{j, 0}$ 表示在 $H_0$ 假设下的系数值（通常为 0）

那么 t 值的含义实际上就是看看**我这个估计值距离零假设的位置有几个标准误差的距离**
> [!note]
> 绝对值越大，认为系数越不可能为 0
### p 值
假如某个系数真的按 $H_0$ 这样等于 0，那么出现这组数据的概率有多大？
- p 很小：$H_0$ 不对
- p 不小：不拒绝 $H_0$
### F 统计量
$H_0$ 变成所有自变量都没用，用来**一次性检验整体是否显著**


> [!note]
> p 值越小，证明参数越显著，系数越不可能为 0
### t 分布
假如零假设真的成立，那么 t 统计量应当服从 t 分布
我们可以用他计算 p 值，查临界值，做显著性检验，画置信区间
### $R^2$ 统计量
公式：
$$
R^2=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}
$$
其中总平方和 TSS（Total Sum of Squares） :
$$
TSS=\Sigma(y_i-\bar{y})
$$
- TSS 表示的是原数据的变异性
- RSS 表示的是回归后仍无法解释的变异性
那么无法解释的变异性占比越小，证明回归的越好

> [!note]
> $R^2$ 越接近 1，回归效果越好

### 调整 $R^2$
$R^2$ 的缺点就是，假如我们引入一个无关变量，就算这个变量对响应变量无关，但是由于自由度增加，相应的残差平方和 RSS 会降低，$R^2$ 会增大。

因此引入调整 $R^2$
$$
R_{adj}^2=1-(\frac{(1-R^2)\cdot(n-1)}{n-p-1})
$$

### 95%置信区间
公式：
$$
估计值±2\cdot SE(估计值) 
$$
表示在这个区间内，有 95%的可能性包含真实值
区间越窄，参数估得越准
### 预测区间
未来一个具体的 y（新样本）会落在哪里
预测区间一定比置信区间宽，因为要考虑两个因素：
- 回归模型的准确性
- 未来 y 的变异性（存在 $\epsilon$ 的波动）

### 线性回归的假设
1. **可加性**：预测变量 $x_j$ 的变化对相应变量 $Y$ 产生的影响**与其他预测变量的取值无关**
2. **线性**：无论 $x_j$ 取什么值，$x_j$ 变化一个单位引起相应变量的变化**是恒定的**
3. 误差项不相关
### 学生化残差
公式：
$$
\frac{e_i}{RSE}
$$
### 离群点
离群点是指对于给定的预测值 $x_i$ 来说，$y_i$ 远离模型预测点的点
- 离群点通常对最小二乘拟合几乎没有影响
- 会影响 RSE、置信区间、p 值、$R^2$
判断离群点：
- 绘制学生化残差图，学生化残差大于 3 的观测点可能是离群点

> [!note]
> x 正常，y 很怪

### 高杠杆点
通过计算**杠杆统计量**，来量化杠杆作用
公式：
$$
h_i=\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\Sigma_{i'=1}^n(x_{i'}-\bar{x})^2}
$$

$h_i$ 的取值在 1/n 到 1 之间，所有观测的平均杠杆值总是等于 $\frac{p+1}{n}$，假如平均值远大于这个值，怀疑这个点具有较高的杠杆作用
> [!note]
> x 脱离大部队

### 误差项自相关
常出现在时序序列数据，也就是说昨天的误差会影响今天的误差，出现**跟踪**现象
### 共线性
预测变量之间高度相关
解决方案：
1. 从回归中剔除一个问题变量
2. 把共线性的变量们组成一个单一的预测变量
### K 最近邻法（KNN）
现在有一堆带标签的数据 ($x_i$，$y_i$)，现在要预测一个新样本 $x_{new}$ 的值 $y_{new}$，KNN 这么做：
1. 在训练样本中，找出离 $x_{new}$ 最近的 K 个点
2. 把它们的 y 值取出来，做平均或者加权平均，这个结果作为 $y_{new}$ 的值
#### 如何选择 K？
- 小的 K 值提供了更灵活的拟合，导致低偏差和高方差
- 大的 K 值自由度更低，比较稳定，方差较小，但可能会隐藏 f（X）的部分结构导致偏差

> [!warning]
> 存在维度灾难的问题，即在高维环境中找不到邻居
> 所以我们优先选择线性回归，即使在低维问题，因为这虽然会损失精度，但是模型简单，p 值清晰，可解释性强

### 逻辑斯蒂回归
$$
p(X)=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}
$$
其范围在 0~1 之间，将分类问题转换为了概率建模的回归问题
我们采用**最大似然方法估计系数**，即求可以最大化某个式子的 $\beta$ 值

变换形式：
$$
log\{\frac{p(X)}{1-p(X)}\}=\beta_0+\beta_1X
$$
转换为了**分对数**变换下关于 X 的一个线性模型
### z 统计量
衡量某个观测值与总体均值之间的偏差
$$
z=\frac{X-\mu}{\sigma}
$$

## 稍后再看
### 贝叶斯分类器